# -*- coding: utf-8 -*-
"""PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MvPUO_rl4OGiY6rghoNm-gLE4gKy2J18
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print(torch.__version__)

"""Introduction Tesors

creating tensors

torch.Tensor -> https://docs.pytorch.org/docs/stable/tensors.html#torch-tensor
"""

# Scalar
scalar = torch.tensor(7)
scalar

scalar.ndim

# Get tensor back as python init
scalar.item()

# Vector
vector = torch.tensor([7, 7])
vector

vector.ndim

vector.shape

# MATRIX
MATRIX = torch.tensor([[7, 8],
                       [9, 10]])
MATRIX

MATRIX.ndim

MATRIX[1]

MATRIX.shape

# TENSOR
TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR[0]

"""### Random tensors"""

5*10*10

## Creating random tensors of sizes
random_tensor = torch.rand(5, 10)
random_tensor

random_tensor.ndim

random_images_sizes_tensors = torch.rand(size=(224, 224)) # hight, weight, colour channel (R, G, B)
random_images_sizes_tensors.shape, random_images_sizes_tensors.ndim

random_images_sizes_tensors = torch.rand(size=(224, 224, 3))
random_images_sizes_tensors.shape, random_images_sizes_tensors.ndim

torch.rand(3, 3)

"""## Zeros and Ones"""

# Create tensorn all zeros
zeros = torch.zeros(size=(3, 4))
zeros

# Create tensorn all ones
ones = torch.ones(size=(3, 4))
ones

ones.dtype # data types

random_tensor.dtype

"""### Creating a range of tensors and tensors-like"""

one_to_ten = torch.arange(1, 11)
one_to_ten

one_to_ten = torch.arange(start=0, end=1000, step=50)
one_to_ten

# createing tensors like
ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros

"""### Tensors data types"""

# Float 32 tensors
float_32_tensors = torch.tensor([3.0, 6.0, 9.0],
                                dtype=None,  # what datatype is the tensor (float32, float16)
                                device=None, # what device is your tensor on
                                requires_grad=False) # whether or not to track gradients with this tensors operations

float_32_tensors

float_32_tensors.dtype

float16_tensors = float_32_tensors.type(torch.float16)
float16_tensors

float16_tensors * float_32_tensors

int_32_tensors = torch.tensor([3, 6, 9], dtype=torch.long)
int_32_tensors

float16_tensors * int_32_tensors

"""### Getting infprmation from tensors"""

# create a tensors
some_tensor = torch.rand(3, 4)
some_tensor

some_tensor.size()

# find out details about some tensors
print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device tensor is on: {some_tensor.device}")

"""### Manipulating Tensors (tensors operations)

involve fundamental arithmetic (addition, multiplication) element-wise or via matrix/tensor products, plus advanced functions like reshaping, transposing, indexing, and slicing

"""

# create a tensors and 10 to it
tensors = torch.tensor([1, 2, 3])
tensors + 10

# Multiply by 10
tensors * 10

tensors

tensors - 10

torch.mul(tensors, 10)

torch.add(tensors, 10)

"""### Matrix multiplication (is all you need)
One of the most common operations in machine learning and deep learning algorithms (like neural networks) is[ matrix multiplication](https://https://www.mathsisfun.com/algebra/matrix-multiplying.html).

PyTorch implements matrix multiplication functionality in the torch.matmul() method.

The main two rules for matrix multiplication to remember are:

The inner dimensions must match:
* (3, 2) @ (3, 2) won't work
* (2, 3) @ (3, 2) will work
* (3, 2) @ (2, 3) will work

The resulting matrix has the shape of the outer dimensions:
* (2, 3) @ (3, 2) -> (2, 2)
* (3, 2) @ (2, 3) -> (3, 3)
Note: "@" in Python is the symbol for matrix multiplication.

Resource: You can see all of the rules for matrix multiplication using torch.matmul() in the PyTorch documentation.

Let's create a tensor and perform element-wise multiplication and matrix multiplication on i
![image.png](https://camo.githubusercontent.com/c4cbd81b1bf62e64a6ea9db18880e7da8e947cfdc4e33c86151a09133b2d4d0a/68747470733a2f2f6769746875622e636f6d2f6d7264626f75726b652f7079746f7263682d646565702d6c6561726e696e672f7261772f6d61696e2f696d616765732f30302d6d61747269782d6d756c7469706c792d63726f702e676966)
"""

# element
print(tensors, "*", tensors)
print(f"Equals: {tensors * tensors}")

# Matrix
torch.matmul(tensors, tensors)

tensors

# Matrix by hand
1*1 + 2*2 + 3*3

# Commented out IPython magic to ensure Python compatibility.
# %%time
# value = 0
# for i in range(len(tensors)):
#   value += tensors[i] * tensors[i]
# print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensors, tensors)

"""### one of most commen error in the deep learing: shape Error

http://matrixmultiplication.xyz/
"""

# sahpe for matrix multiplication

tensor_A = torch.tensor([[1, 2],
                         [3, 4],
                         [5, 6]])

tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

# torch.mm(tensor_A, tensor_B) # torch.mm is the same as torch.matmul (it's an alias for torch.matmul

tensor_A.shape, tensor_B.shape

tensor_B, tensor_B.shape

tensor_B.T, tensor_B.T.shape

torch.mm(tensor_A, tensor_B.T)

print(f"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}")
print(f"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}")
print(f"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match")
print("Output:\n")
output = torch.mm(tensor_A, tensor_B.T)
print(output)
print(f"\nOutput shape: {output.shape}")

"""### Finding the min, max, mean, sum etc (tensors aggregation)"""

# create a tensors
x = torch.arange(1, 100, 10)
x, x.dtype

# Find the min
torch.min(x), x.min()

# Find the max
torch.max(x), x.max()

# Find the mean
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()

# Find the sum
torch.sum(x), x.sum()

"""### Find the positional min and max"""

x

# Find the positions in a tensor that has the minimum value with argmin() => return index
x.argmin()

x[0]

# Find the position in tensor that has the maximum value with argmax()
x.argmax()

x[9]

"""### Reshaping, stacking, squeezing and unsqueezing tensors

| Method | One-line description |
|-------|----------------------|
| `torch.reshape(input, shape)` | Reshapes `input` to `shape` (if compatible); can also use `torch.Tensor.reshape()` |
| `Tensor.view(shape)` | Returns a view of the original tensor in a different shape, sharing the same underlying data |
| `torch.stack(tensors, dim=0)` | Concatenates a sequence of tensors along a **new** dimension `dim`; all tensors must have the same size |
| `torch.squeeze(input)` | Removes all dimensions of size `1` from the tensor |
| `torch.unsqueeze(input, dim)` | Adds a dimension of size `1` at the specified `dim` |
| `torch.permute(input, dims)` | Returns a view of the tensor with its dimensions rearranged according to `dims` |


"""

# Create a tensors
import torch
x = torch.arange(1., 10.)
x, x.shape

# Add a extra dimension
x_reshaped = x.reshape(1, 9)
x_reshaped, x_reshaped.shape

# change the view
z = x.view(1, 9)
z, z.shape

# changes z changes x
z[:, 0] = 5
z, x

# Stack tensors on the top of each orthers
x_stacked = torch.stack([x, x, x, x], dim=0)
x_stacked

print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape: {x_reshaped.shape}")

# Remove extra dimension from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}")
print(f"New shape: {x_squeezed.shape}")

print(f"Previous tensor: {x_squeezed}")
print(f"Previous shape: {x_squeezed.shape}")

## Add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=0)
print(f"\nNew tensor: {x_unsqueezed}")
print(f"New shape: {x_unsqueezed.shape}")

x_original = torch.rand(size=(224, 224, 3)) # [height, width, colour_channel]

x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0

print(f"Previous shape: {x_original.shape}")
print(f"New shape: {x_permuted.shape}")

x_original[0, 0, 0] = 728218
x_original[0, 0, 0], x_permuted[0, 0, 0]

"""### Indexing (Select data from tensors)
indexing with pythorch is similer to indexing with numpy
"""

# Create a tensors
import torch
x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape

# Let index on our new tensors
x[0]

# let index on the middel bracket (dim=1)
x[0][0]

# Lets index on the most inner bracket (last dimension)
x[0][0][0]

x[0][2][2]

# we can also use ":" to select "all" of a target dimension
x[:, 0]

x[:, :, 1]

x[:, 2, 2]

# Get index 0 of 0th and 1st dimnsion and all values of 2nd dimension
x[0, 0, :]

# Index on return on number 9
x[:, 2, 2]

# Index on x to return 3, 6, 9
x[:, :, 2]

x

"""## Pythorch tensors and numpy

"""

# numpy array
import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensors = torch.from_numpy(array)
array, tensors

array.dtype

torch.arange(1.0, 8.0).dtype

array = array + 1
array, tensors

# TENSORS TO NUMPY
tensors = torch.ones(7)
numpy_tensors = tensors.numpy()
tensors, numpy_tensors

"""### Reproducbility (traying to take random out of random)"""

import torch
# Create two random numbers
random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)



